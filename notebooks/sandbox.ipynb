{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.data_loading import train_users\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_users = train_users.head(100)\n",
    "y_train = train_users['country_destination']\n",
    "train_users.drop('country_destination', axis=1, inplace=True)\n",
    "train_users.drop('id', axis=1, inplace=True)\n",
    "train_users = train_users.fillna(-1)\n",
    "\n",
    "x_train = train_users.values\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Multiclass and multilabel classification strategies.\n",
    "\n",
    "This module implements an one-vs-one multiclass learning algorithm that uses\n",
    "SMOTE algorithm to over sample the minority class in each fit.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import _fit_binary, _predict_binary, _ovr_decision_function\n",
    "from sklearn.multiclass import check_is_fitted\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from utils.unbalanced_dataset import SMOTE\n",
    "from utils.unbalanced_dataset import SMOTEENN\n",
    "\n",
    "\n",
    "def _fit_ovo_binary(estimator, X, y, i, j, sampling=None, verbose=False):\n",
    "    \"\"\"Fit a single binary estimator (one-vs-one).\"\"\"\n",
    "    cond = np.logical_or(y == i, y == j)\n",
    "    y = y[cond]\n",
    "    y_binary = np.empty(y.shape, np.int)\n",
    "    y_binary[y == i] = 0\n",
    "    y_binary[y == j] = 1\n",
    "    ind = np.arange(X.shape[0])\n",
    "\n",
    "    X_values = X[ind[cond]]\n",
    "    y_values = y_binary\n",
    "\n",
    "    if sampling:\n",
    "        ones = np.count_nonzero(y_values == 1)\n",
    "        zeros = np.count_nonzero(y_values == 0)\n",
    "\n",
    "        if sampling == 'SMOTE':\n",
    "            ratio = abs(ones - zeros) / min(ones, zeros)\n",
    "            smote = SMOTE(ratio=ratio, verbose=verbose)\n",
    "\n",
    "        if sampling == 'SMOTEENN':\n",
    "            ratio = (abs(ones - zeros) / min(ones, zeros)) * 0.3\n",
    "            smote = SMOTEENN(ratio=ratio, verbose=verbose)\n",
    "\n",
    "        X_values, y_values = smote.fit_transform(X_values, y_values)\n",
    "\n",
    "    return _fit_binary(estimator, X_values, y_values, classes=[i, j])\n",
    "\n",
    "\n",
    "class CustomOneVsOneClassifier(OneVsOneClassifier):\n",
    "    \"\"\"One-vs-one multiclass strategy.\n",
    "\n",
    "    This strategy consists in fitting one classifier per class pair.\n",
    "    At prediction time, the class which received the most votes is selected.\n",
    "\n",
    "    Requires to fit `n_classes * (n_classes - 1) / 2` classifiers.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_ : list of `n_classes * (n_classes - 1) / 2` estimators\n",
    "        Estimators used for predictions.\n",
    "    classes_ : numpy array of shape [n_classes]\n",
    "        Array containing labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, n_jobs=1, sampling=None, verbose=False):\n",
    "        \"\"\"Init method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : estimator object\n",
    "            An estimator object implementing fit and one of decision_function\n",
    "            or predict_proba.\n",
    "        n_jobs : int, optional, default: 1\n",
    "            The number of jobs to use. If -1 all CPUs are used.\n",
    "            If 1 is given, no parallel computing code is used at all, which is\n",
    "            useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs)\n",
    "            are used. Thus for n_jobs = -2, all CPUs but one are used.\n",
    "        sampling : str, optional default:None\n",
    "            Samplig method to use when fitting each estimator.\n",
    "            Can be 'SMOTE' or SMOTEENN'.\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "        self.n_jobs = n_jobs\n",
    "        self.sampling = sampling\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "\n",
    "        The predicted class probabilities of an input sample is the same as\n",
    "        the result of decision_function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
    "            such arrays if n_outputs > 1.\n",
    "            The class probabilities of the input samples.\n",
    "        \"\"\"\n",
    "        return self.decision_function(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit underlying estimators.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : (sparse) array-like, shape = [n_samples, n_features]\n",
    "            Data.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Multi-class targets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_classes = self.classes_.shape[0]\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_ovo_binary)(\n",
    "                self.estimator, X, y, self.classes_[i], self.classes_[j],\n",
    "                sampling=self.sampling, verbose=self.verbose\n",
    "            ) for i in range(n_classes) for j in range(i + 1, n_classes))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Decision function for the OneVsOneClassifier.\n",
    "        The decision values for the samples are computed by adding the\n",
    "        normalized sum of pair-wise classification confidence levels to the\n",
    "        votes in order to disambiguate between the decision values when the\n",
    "        votes for all the classes are equal leading to a tie.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        Y : array-like, shape = [n_samples, n_classes]\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "        predictions = np.vstack([est.predict(X) for est in self.estimators_]).T\n",
    "        confidences = np.vstack([_predict_binary(est, X) for est in self.estimators_]).T\n",
    "        return _ovr_decision_function(predictions, confidences,\n",
    "                                      len(self.classes_))\n",
    "\n",
    "def _ovr_decision_function(predictions, confidences, n_classes):\n",
    "    \"\"\"Compute a continuous, tie-breaking ovr decision function.\n",
    "    It is important to include a continuous value, not only votes,\n",
    "    to make computing AUC or calibration meaningful.\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : array-like, shape (n_samples, n_classifiers)\n",
    "        Predicted classes for each binary classifier.\n",
    "    confidences : array-like, shape (n_samples, n_classifiers)\n",
    "        Decision functions or predicted probabilities for positive class\n",
    "        for each binary classifier.\n",
    "    n_classes : int\n",
    "        Number of classes. n_classifiers must be\n",
    "        ``n_classes * (n_classes - 1 ) / 2``\n",
    "    \"\"\"\n",
    "    n_samples = predictions.shape[0]\n",
    "    votes = np.zeros((n_samples, n_classes))\n",
    "    sum_of_confidences = np.zeros((n_samples, n_classes))\n",
    "\n",
    "    k = 0\n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            sum_of_confidences[:, i] -= confidences[:, k]\n",
    "            sum_of_confidences[:, j] += confidences[:, k]\n",
    "            votes[predictions[:, k] == 0, i] += 1\n",
    "            votes[predictions[:, k] == 1, j] += 1\n",
    "            k += 1\n",
    "\n",
    "    max_confidences = sum_of_confidences.max()\n",
    "    min_confidences = sum_of_confidences.min()\n",
    "\n",
    "    if max_confidences == min_confidences:\n",
    "        return votes\n",
    "\n",
    "    # Scale the sum_of_confidences to (-0.5, 0.5) and add it with votes.\n",
    "    # The motivation is to use confidence levels as a way to break ties in\n",
    "    # the votes without switching any decision made based on a difference\n",
    "    # of 1 vote.\n",
    "    eps = np.finfo(sum_of_confidences.dtype).eps\n",
    "    max_abs_confidence = max(abs(max_confidences), abs(min_confidences))\n",
    "    scale = (0.5 - eps) / max_abs_confidence\n",
    "    print votes\n",
    "    return votes + sum_of_confidences * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "clf = CustomOneVsOneClassifier(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomOneVsOneClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "             n_jobs=1, sampling=None, verbose=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, encoded_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  0.  5.  2.  3.  7.  6.  1.]\n",
      " [ 4.  0.  5.  3.  1.  7.  6.  2.]\n",
      " [ 3.  0.  5.  3.  3.  6.  7.  1.]\n",
      " [ 2.  0.  5.  1.  3.  7.  5.  5.]\n",
      " [ 4.  0.  5.  2.  3.  6.  7.  1.]\n",
      " [ 5.  0.  4.  2.  2.  6.  7.  2.]\n",
      " [ 3.  1.  5.  3.  1.  6.  7.  2.]\n",
      " [ 3.  1.  5.  3.  1.  6.  7.  2.]\n",
      " [ 3.  1.  5.  4.  1.  6.  7.  1.]\n",
      " [ 3.  0.  4.  4.  3.  6.  7.  1.]]\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict_proba(x_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NDF' 'NDF' 'US' 'other' 'US' 'US' 'US' 'US' 'US' 'US']\n",
      "[5 5 6 7 6 6 6 6 6 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 6, 2, 0, 3, 7, 1, 4],\n",
       "       [5, 6, 2, 0, 7, 3, 1, 4],\n",
       "       [6, 5, 2, 0, 7, 3, 1, 4],\n",
       "       [7, 5, 6, 2, 0, 3, 4, 1],\n",
       "       [6, 5, 2, 7, 3, 0, 4, 1],\n",
       "       [6, 5, 2, 3, 0, 7, 1, 4],\n",
       "       [6, 5, 2, 0, 7, 1, 3, 4],\n",
       "       [6, 5, 2, 0, 7, 1, 3, 4],\n",
       "       [6, 5, 2, 0, 7, 1, 3, 4],\n",
       "       [6, 5, 2, 7, 3, 0, 1, 4]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y_train.values[0:10]\n",
    "print encoded_y_train[0:10]\n",
    "np.argsort(preds * -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
